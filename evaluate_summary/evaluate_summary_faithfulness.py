import json
import os

import utils
from evaluate_summary import evaluate_summary_utils
from evaluate_summary.evaluate_summary_params import EvaluateSummaryParams
from summarize_transcripts import summarize_transcripts_utils
from transcribe_audio_chunks import transcribe_audio_chunks_utils


def evaluate(
    evaluate_summary_params: EvaluateSummaryParams,
):
    print(f"evaluate faithfulness of summary...")

    # handle already done

    faithfulness_evaluation_path = (
        evaluate_summary_utils.get_faithfulness_evaluation_path(
            evaluate_summary_params=evaluate_summary_params
        )
    )

    if os.path.isfile(faithfulness_evaluation_path):
        print(f"...faithfulness of summary already evaluated")
        return

    # create system prompt

    system_prompt = """
        You are an AI judge that evaluates if a summary is a faithful representation of multiple transcript snippets.
        
        YOU WILL RECEIVE:
        
        1. Several consecutive transcript snippets from the same video.
        2. A summary of those transcripts generated by an AI model.
        
        YOUR TASK:
        
        Evaluate if the summary made up facts or states facts that contradict the transcripts.
        Minor inaccuracies are allowed.
        Give a short explanation (2 sentences) and the result of your analysis (True if its faithful, False otherwise).
        
        OUTPUT FORMAT:
        
        Do only output the following JSON
        
        {
            "explanation": "string",
            "is_faithful: "boolean"
        }
    """

    # create user prompt

    user_prompt_with_placeholders = """
        TRANSCRIPT SNIPPETS:
        
        {{transcripts}}
    
        SUMMARY:
        
        {{summary}}
    """

    summary_placeholder_value = summarize_transcripts_utils.get_summary(
        summarize_transcripts_params=evaluate_summary_params.summarize_transcripts_params,
    )

    transcripts = transcribe_audio_chunks_utils.get_transcripts(
        evaluate_summary_params.transcribe_audio_chunks_params
    )

    transcripts_placeholder_value = "\n\n".join(
        [
            f"Transcript Snippet {transcript_idx}:\n\n{transcript}"
            for transcript_idx, transcript in enumerate(transcripts)
        ]
    )

    user_prompt = user_prompt_with_placeholders.replace(
        "{{transcripts}}", transcripts_placeholder_value
    ).replace("{{summary}}", summary_placeholder_value)

    # perform and save faithfulness evaluation as json

    faithfulness_evaluation_response = (
        evaluate_summary_params.llm_evaluator_system_and_user_prompt_to_response(
            system_prompt,
            user_prompt,
        )
    )

    faithfulness_evaluation_as_json = json.loads(faithfulness_evaluation_response)

    utils.save_json(
        path=faithfulness_evaluation_path,
        json_for_saving=faithfulness_evaluation_as_json,
    )

    print(f"...evaluated faithfulness of summary")
