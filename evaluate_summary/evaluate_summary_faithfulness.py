import os
import json
from typing import Callable
import utils
from chunk_audio.chunk_audio_params import ChunkAudioParams
from evaluate_summary import evaluate_summary_utils
from summarize_transcripts import summarize_transcripts_utils
from summarize_transcripts.summarize_transcripts_params import (
    SummarizeTranscriptsParams,
)
from transcribe_audio_chunks import transcribe_audio_chunks_utils
from transcribe_audio_chunks.transcribe_audio_chunks_params import (
    TranscribeAudioChunksParams,
)


def evaluate(
    base_path: str,
    chunk_audio_params: ChunkAudioParams,
    transcribe_audio_chunks_params: TranscribeAudioChunksParams,
    summarize_transcripts_params: SummarizeTranscriptsParams,
    llm_evaluator_system_and_user_prompt_to_response: Callable[[str, str], str],
):
    print(f"evaluate faithfulness of summary...")

    # handle already done

    faithfulness_evaluation_path = (
        evaluate_summary_utils.get_faithfulness_evaluation_path(
            base_path=base_path,
            chunk_audio_params=chunk_audio_params,
            transcribe_audio_chunks_params=transcribe_audio_chunks_params,
            summarize_transcripts_params=summarize_transcripts_params,
        )
    )

    if os.path.isfile(faithfulness_evaluation_path):
        print(f"...faithfulness of summary already evaluated")
        return

    # create system prompt

    system_prompt = """
        You are an AI judge that evaluates if a summary is a faithful representation of multiple transcript snippets.
        
        YOU WILL RECEIVE:
        
        1. Several consecutive transcript snippets from the same video.
        2. A summary of those transcripts generated by an AI model.
        
        YOUR TASK:
        
        Evaluate if the summary made up facts or states facts that contradict the transcripts.
        Minor inaccuracies are allowed.
        Give a short explanation (2 sentences) and the result of your analysis (True if its faithful, False otherwise).
        
        OUTPUT FORMAT:
        
        Do only output the following JSON
        
        {
            "explanation": "string",
            "is_faithful: "boolean"
        }
    """

    # create user prompt

    user_prompt_with_placeholders = """
        TRANSCRIPT SNIPPETS:
        
        {{transcripts}}
    
        SUMMARY:
        
        {{summary}}
    """

    summary_placeholder_value = summarize_transcripts_utils.get_summary(
        base_path=base_path,
        chunk_audio_params=chunk_audio_params,
        transcribe_audio_chunks_params=transcribe_audio_chunks_params,
        summarize_transcripts_params=summarize_transcripts_params,
    )

    transcripts = transcribe_audio_chunks_utils.get_transcripts(
        base_path=base_path, chunk_audio_params=chunk_audio_params
    )

    transcripts_placeholder_value = "\n\n".join(
        [
            f"Transcript Snippet {transcript_idx}:\n\n{transcript}"
            for transcript_idx, transcript in enumerate(transcripts)
        ]
    )

    user_prompt = user_prompt_with_placeholders.replace(
        "{{transcripts}}", transcripts_placeholder_value
    ).replace("{{summary}}", summary_placeholder_value)

    # perform and save faithfulness evaluation as json

    faithfulness_evaluation_response = llm_evaluator_system_and_user_prompt_to_response(
        system_prompt,
        user_prompt,
    )

    faithfulness_evaluation_as_json = json.loads(faithfulness_evaluation_response)

    utils.save_json(
        path=faithfulness_evaluation_path,
        json_for_saving=faithfulness_evaluation_as_json,
    )

    print(f"...evaluated faithfulness of summary")
